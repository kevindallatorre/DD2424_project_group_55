{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self,nr_of_classes=4,image_shape =(32, 32, 3)):\n",
    "        self.mean=np.zeros([3])\n",
    "        self.std=np.zeros([3]) \n",
    "        self.nr_of_classes=nr_of_classes\n",
    "        self.nomalize_data=True\n",
    "        self.image_shape=image_shape\n",
    "        self.x = tf.placeholder(tf.float32, shape=(None,)+self.image_shape)\n",
    "        self.y = tf.placeholder(tf.int32)\n",
    "        self.Y = tf.placeholder(tf.float32, shape=(None,self.nr_of_classes))\n",
    "        self.x_rot =  tf.image.rot90(self.x, self.y)\n",
    "        \n",
    "\n",
    "    def set_params(self,nr_of_classes=None,input_shape=None,norm_params=None,img=None):\n",
    "        if nr_of_classes!=None:    self.nr_of_classes=nr_of_classes                 \n",
    "        if input_shape!=None:      self.input_shape=input_shape\n",
    "        if norm_params!=None and img!=None:\n",
    "            img_array = np.asarray(img)\n",
    "            for i in range(3):\n",
    "                self.mean[i]=img_array[:,:,:,i].mean()\n",
    "                self.std[i]=img_array[:,:,:,i].std()\n",
    "        elif norm_params!=None and img==None:\n",
    "              print(\"Data is required\")\n",
    "\n",
    "\n",
    "    def normalize(self,img,img2):\n",
    "        img_array = np.asarray(img)\n",
    "        img_array2 = np.asarray(img2)\n",
    "        normalized = np.zeros(np.shape(img_array))\n",
    "        normalized2 = np.zeros(np.shape(img_array2))\n",
    "        if self.nomalize_data:\n",
    "            for i in range(3):\n",
    "                self.mean[i]=img_array[:,:,:,i].mean()\n",
    "                self.std[i]=img_array[:,:,:,i].std()\n",
    "        for i in range(3):\n",
    "            normalized = (img_array - self.mean[i]) / (self.std[i])\n",
    "            normalized2 = (img_array2 - self.mean[i]) / (self.std[i])\n",
    "        return normalized,normalized2\n",
    "    \n",
    "\n",
    "    def get_one_hot_rot(self,x,K):\n",
    "        one_hot=np.zeros((np.shape(x)[0],K))\n",
    "        one_hot[:,0]=1\n",
    "        for j in range(K-1):\n",
    "            labels=np.zeros((np.shape(x)[0],K))\n",
    "            labels[:,j+1]=1\n",
    "            one_hot=np.concatenate((one_hot,labels))\n",
    "        return one_hot\n",
    "    \n",
    "    \n",
    "    def get_one_hot(self,y,K):\n",
    "        one_hot=np.zeros((np.shape(y)[0],K))\n",
    "        for j in range(np.shape(y)[0]):\n",
    "            one_hot[j,y[j]]=1\n",
    "        return one_hot\n",
    "    \n",
    "    \n",
    "    def create_rotated_data(self,x_train,x_test,nomalize_data=False):\n",
    "        if self.nomalize_data:\n",
    "            x_train,x_test=self.normalize(x_train,x_test)\n",
    "            self.nomalize_data=False\n",
    "                      \n",
    "        elif nomalize_data==True:\n",
    "            x_train,x_test=self.normalize(x_train,x_test)\n",
    "            \n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess: \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            labels=self.get_one_hot_rot(x_train,self.nr_of_classes)\n",
    "            x_train_rot=sess.run ( self.x_rot, feed_dict={self.x:x_train,self.y:0})\n",
    "            for j in range(self.nr_of_classes-1):\n",
    "                x_train_rot=np.concatenate((x_train_rot,sess.run ( self.x_rot, feed_dict={self.x:x_train, self.y:(j+1)})))\n",
    "            x_test_rot=sess.run ( self.x_rot, feed_dict={self.x:x_test,self.y:0})\n",
    "            labels_test=self.get_one_hot_rot(x_test,self.nr_of_classes)\n",
    "            for j in range(self.nr_of_classes-1):\n",
    "                x_test_rot=np.concatenate((x_test_rot,sess.run ( self.x_rot, feed_dict={self.x:x_test, self.y:(j+1)})))                \n",
    "                \n",
    "        return [x_train_rot,labels,x_test_rot,labels_test]\n",
    "      \n",
    "      \n",
    "      \n",
    "    def create_data(self,x_train,y_train,x_test,y_test,nomalize_data=False):\n",
    "        if self.nomalize_data:\n",
    "            x_train,x_test=self.normalize(x_train,x_test)\n",
    "            self.nomalize_data=False\n",
    "\n",
    "        elif nomalize_data==True:\n",
    "            x_train,x_test=self.normalize(x_train,x_test)\n",
    "\n",
    "        labels=self.get_one_hot(y_train,self.nr_of_classes)\n",
    "        labels_test=self.get_one_hot(y_test,self.nr_of_classes)\n",
    "        return [x_train,labels,x_test,labels_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWorkInNetwork:\n",
    "    \n",
    "    def __init__(self,model=None,nr_of_classes=4,nr_of_blocks=4,DECAY = 5e-4 ,input_shape=(32,32,3),conv__layer_strides=[1,1],pooling_layer_strides=(2,2)):\n",
    "        if model==None:\n",
    "            self.model=tf.keras.models.Sequential()\n",
    "            self.cov_1_created=False\n",
    "        else:\n",
    "            self.model=model\n",
    "            self.cov_1_created=True\n",
    "\n",
    "        self.nr_of_classes=nr_of_classes\n",
    "        self.nr_of_blocks=nr_of_blocks\n",
    "        self.DECAY = DECAY\n",
    "        self.conv_strides=conv__layer_strides\n",
    "        self.pooling_strides=pooling_layer_strides\n",
    "        self.input_shape=input_shape\n",
    "        \n",
    "        \n",
    "    def set_params(nr_of_classes=None,nr_of_blocks=None,DECAY = None ,input_shape=None,conv__layer_strides=None,pooling_layer_strides=None):\n",
    "        \n",
    "        if nr_of_classes!=None:    self.nr_of_classes=nr_of_classes        \n",
    "        if nr_of_blocks!=None:     self.nr_of_blocks=nr_of_blocks      \n",
    "        if DECAY!=None:            self.DECAY = DECAY       \n",
    "        if input_shape!=None:      self.input_shape=input_shape\n",
    "        if conv_strides!=None:     self.conv_strides=conv_strides        \n",
    "        if pooling_strides!=None:  self.pooling_strides = pooling_strides\n",
    "        \n",
    "        \n",
    "    def conv_layer(self,kernel_size,out_channels):\n",
    "        \n",
    "        my_init=tf.keras.initializers.RandomNormal(mean=0.0, stddev=tf.math.sqrt(2/(kernel_size*kernel_size*out_channels)))\n",
    "        if not self.cov_1_created:\n",
    "            self.cov_1_created=True\n",
    "            self.model.add(tf.keras.layers.Conv2D(out_channels, kernel_size=kernel_size,strides=self.conv_strides,\n",
    "                                                  kernel_initializer=my_init,\n",
    "                                                  kernel_regularizer=tf.keras.regularizers.l2(self.DECAY), \n",
    "                                                  use_bias=False ,\n",
    "                                                  input_shape=self.input_shape,\n",
    "                                                  padding='same',\n",
    "                                                  data_format='channels_last'))         \n",
    "        else:\n",
    "            self.model.add(tf.keras.layers.Conv2D(out_channels, kernel_size=kernel_size,strides=self.conv_strides,\n",
    "                                                  kernel_initializer=my_init,\n",
    "                                                  kernel_regularizer=tf.keras.regularizers.l2(self.DECAY),\n",
    "                                                  use_bias=False ,\n",
    "                                                  padding='same'))\n",
    "        \n",
    "        self.model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05))\n",
    "        self.model.add(tf.keras.layers.ReLU())\n",
    "                           \n",
    "                           \n",
    "        \n",
    "    def create_block(self,block_type=\"middle\",kernel_list=[3,1,1],channels_list=[192,192,192]):\n",
    "                           \n",
    "        self.conv_layer(kernel_list[0],channels_list[0])\n",
    "        self.conv_layer(kernel_list[1],channels_list[1])\n",
    "        self.conv_layer(kernel_list[2],channels_list[2])\n",
    "                   \n",
    "        if block_type==\"first\":\n",
    "                self.model.add(tf.keras.layers.ZeroPadding2D(padding=(1, 1)))\n",
    "                self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=self.pooling_strides))\n",
    "                           \n",
    "                           \n",
    "        elif block_type==\"last\":\n",
    "            self.model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "            my_init_dense=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='uniform')\n",
    "            self.model.add(tf.keras.layers.Dense(self.nr_of_classes, activation=tf.nn.softmax,\n",
    "                                                 kernel_initializer=my_init_dense,\n",
    "                                                 kernel_regularizer=tf.keras.regularizers.l2(self.DECAY),\n",
    "                                                 bias_initializer='zeros',\n",
    "                                                 bias_regularizer=tf.keras.regularizers.l2(self.DECAY)))\n",
    "                        \n",
    "        else:\n",
    "            self.model.add(tf.keras.layers.ZeroPadding2D(padding=(1, 1)))\n",
    "            self.model.add(tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=self.pooling_strides))\n",
    "                           \n",
    "                           \n",
    "    def create_network(self,nr_of_blocks=None,return_model=False):\n",
    "        if nr_of_blocks==None:\n",
    "            nr_of_blocks=self.nr_of_blocks\n",
    "    \n",
    "        self.create_block(block_type=\"first\",kernel_list=[5,1,1],channels_list=[192,160,96])\n",
    "        self.create_block(kernel_list=[5,1,1])\n",
    "        if nr_of_blocks>2:\n",
    "            for i in range(3,nr_of_blocks):\n",
    "                self.create_block() \n",
    "        self.create_block(block_type=\"last\")\n",
    "        if return_model:\n",
    "            return self.model\n",
    "        \n",
    "    def train_model(self):\n",
    "        my_optimizer=tf.keras.optimizers.SGD(momentum=MOMENTUM, nesterov=True)\n",
    "        self.model.compile(optimizer=my_optimizer,loss=cross_entropy_loss,metrics=['accuracy'])\n",
    "        self.model.fit(batch_x_rot,labels, batch_size=512, epochs=100, callbacks=callbacks_list)\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_and_Evaluate:\n",
    "    def __init__(self, model, epochs=100, batch_size=128, Momentum=0.9, Decay=5e-4,initial_lrate=0.1,lr_decay=5,limit_1=30,limit_2=60,limit_3=80,limit_4=None):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.Momentum = Momentum\n",
    "        self.Losshistory=self.LossHistory(self)\n",
    "        self.initial_lrate=float(initial_lrate)\n",
    "        self.lr_decay=lr_decay\n",
    "        self.limit_1=limit_1\n",
    "        self.limit_2=limit_2\n",
    "        self.limit_3=limit_3\n",
    "        if limit_4!=None:\n",
    "            self.limit_4=limit_4\n",
    "        else:\n",
    "            self.limit_4=epochs+1\n",
    "        \n",
    "    def cross_entropy_loss(self, F, Y):\n",
    "        return -tf.math.reduce_mean(tf.math.log(tf.reduce_sum(F * Y, 1)), 0)\n",
    "\n",
    "    def step_decay(self, epoch):\n",
    "        if epoch <=self.limit_1:\n",
    "            lrate = self.initial_lrate\n",
    "        elif epoch <=self.limit_2:\n",
    "            lrate = self.initial_lrate / self.lr_decay\n",
    "        elif epoch <=self.limit_3:\n",
    "            lrate = self.initial_lrate / pow(self.lr_decay,2)\n",
    "        elif epoch <=self.limit_4:\n",
    "            lrate = self.initial_lrate / pow(self.lr_decay,3)   \n",
    "        else:\n",
    "            lrate = self.initial_lrate / pow(self.lr_decay,4)\n",
    "        return lrate\n",
    "    \n",
    "    def set_step_decay(self,initial_lrate=None, lr_decay=None,limit_1=None,limit_2=None,limit_3=None,limit_4=None):\n",
    "        if initial_lrate!=None: self.initial_lrate=float(initial_lrate)\n",
    "        if lr_decay!=None: self.lr_decay=lr_decay\n",
    "        if limit_1!=None:  self.limit_1=limit_1\n",
    "        if limit_2!=None:  self.limit_2=limit_2\n",
    "        if limit_3!=None:  self.limit_3=limit_3\n",
    "        if limit_4!=None:  self.limit_4=limit_4\n",
    "        \n",
    "    \n",
    "    class LossHistory(tf.keras.callbacks.Callback):\n",
    "    \n",
    "        def __init__(self,train_and_evaluate):\n",
    "            self.train_and_evaluate=train_and_evaluate\n",
    "\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.losses = []\n",
    "            self.lr = []\n",
    "\n",
    "        def on_epoch_end(self, batch, logs={}):\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.lr.append(self.train_and_evaluate.step_decay(len(self.losses)))\n",
    "            print('lr:', self.train_and_evaluate.step_decay(len(self.losses)))\n",
    "\n",
    "    \n",
    "    def train(self,x_train,labels):\n",
    "        loss_history = self.Losshistory\n",
    "        lrate = tf.keras.callbacks.LearningRateScheduler(self.step_decay)\n",
    "        callbacks_list = [loss_history, lrate]\n",
    "        my_optimizer = tf.keras.optimizers.SGD(momentum=self.Momentum, nesterov=True)\n",
    "        self.model.compile(optimizer=my_optimizer, loss=self.cross_entropy_loss, metrics=['accuracy'])\n",
    "        self.model.fit(x_train, labels, batch_size=self.batch_size, epochs=self.epochs, callbacks=callbacks_list)\n",
    "\n",
    "    def evaluate(self, x_test, labels, get_value=False):\n",
    "        loss, acc = self.model.evaluate(x_test, labels)\n",
    "        if get_value:\n",
    "            return loss, acc\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transfer_learning(Train_and_Evaluate):   \n",
    "\n",
    "    def __init__(self,model,classifier=\"nonlinear\",blocks=2,freez=True,old_model_blocks=4):\n",
    "        \n",
    "        if isinstance(model, str):\n",
    "            # When we trained our models, we used the name loss_kev for the cross_entropy_loss that we defined.\n",
    "            # So by using try and except we can load both the models that where trained before and after we\n",
    "            # changed the name of the loss without retraining them.  \n",
    "            try:\n",
    "                self.model = tf.keras.models.load_model(model,custom_objects={'cross_entropy_loss': super().cross_entropy_loss})\n",
    "            except:\n",
    "                self.model = tf.keras.models.load_model(model,custom_objects={'loss_kev': super().cross_entropy_loss})\n",
    "        else:\n",
    "            self.model=model\n",
    "\n",
    "        self.classifier=classifier\n",
    "        self.blocks=blocks\n",
    "        self.freez=freez\n",
    "        self.old_model_blocks=old_model_blocks\n",
    "\n",
    "    def add_nonlinear_classifier(self):\n",
    "        my_init_dense=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='normal')\n",
    "        self.model.add(tf.keras.layers.Flatten())\n",
    "        self.model.add(tf.keras.layers.Dense(200, activation=None,kernel_initializer=my_init_dense, kernel_regularizer=tf.keras.regularizers.l2(5e-4),use_bias=False))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05, name='bn1'))\n",
    "        self.model.add(tf.keras.layers.ReLU(name='relu1'))\n",
    "        self.model.add(tf.keras.layers.Dense(200, activation=None,kernel_initializer=my_init_dense, kernel_regularizer=tf.keras.regularizers.l2(5e-4),use_bias=False))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05, name='bn2'))\n",
    "        self.model.add(tf.keras.layers.ReLU(name='relu2'))\n",
    "        self.model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax,kernel_initializer=my_init_dense, kernel_regularizer=tf.keras.regularizers.l2(5e-4), bias_initializer='zeros',bias_regularizer=tf.keras.regularizers.l2(5e-4)))\n",
    "\n",
    "\n",
    "    def add_convolutional_classifier(self):\n",
    "        my_init_4=tf.keras.initializers.RandomNormal(mean=0.0, stddev=np.math.sqrt(2.0/192))\n",
    "        my_init_5=tf.keras.initializers.RandomNormal(mean=0.0, stddev=np.math.sqrt(2.0/(3*3*192)))\n",
    "        my_init_dense=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='normal')\n",
    "        self.model.add(tf.keras.layers.Conv2D(192,kernel_size=3,strides=[1,1], kernel_initializer=my_init_5,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same', name='conv1'))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05, name='bn1'))\n",
    "        self.model.add(tf.keras.layers.ReLU(name='relu1'))\n",
    "        self.model.add(tf.keras.layers.Conv2D(192,kernel_size=1,strides=[1,1], kernel_initializer=my_init_4,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same', name='conv2'))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05, name='bn2'))\n",
    "        self.model.add(tf.keras.layers.ReLU(name='relu2'))\n",
    "        self.model.add(tf.keras.layers.Conv2D(192,kernel_size=1,strides=[1,1], kernel_initializer=my_init_4,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same', name='conv3'))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05, name='bn3'))\n",
    "        self.model.add(tf.keras.layers.ReLU(name='relu3'))\n",
    "        self.model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "        self.model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax,kernel_initializer=my_init_dense, kernel_regularizer=tf.keras.regularizers.l2(5e-4), bias_initializer='zeros',bias_regularizer=tf.keras.regularizers.l2(5e-4)))\n",
    "\n",
    "    def transfer_algorithm(self,freez=True):\n",
    "        self.freez=freez\n",
    "        add_padding_and_avg_pol=False\n",
    "        if self.blocks < self.old_model_blocks:\n",
    "            layer_index = self.blocks * 11\n",
    "        else:\n",
    "            layer_index = self.blocks*11-2\n",
    "            add_padding_and_avg_pol = True\n",
    "        new_model = tf.keras.models.Sequential()\n",
    "        for i in range(layer_index):\n",
    "            layer = self.model.get_layer(index=i)\n",
    "            if self.freez:\n",
    "                layer.trainable = False\n",
    "            new_model.add(layer)\n",
    "            \n",
    "        if add_padding_and_avg_pol:\n",
    "            new_model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05))\n",
    "            new_model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "        self.model=new_model\n",
    "        if self.classifier == \"nonlinear\":\n",
    "            self.add_nonlinear_classifier()\n",
    "        elif self.classifier == \"convolutional\":\n",
    "            self.add_convolutional_classifier()\n",
    "        else:\n",
    "            print(\"One of the strings nonlinear or convolutional is required\")\n",
    "        return self.model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "process_data=Preprocessing()\n",
    "x_train_rot,labels,x_test_rot,labels_test=process_data.create_rotated_data(x_train,x_test)\n",
    "\n",
    "RotNet=NetWorkInNetwork()\n",
    "RotNet.create_network()\n",
    "RotNet=RotNet.get_model()\n",
    "train_RotNet=Train_and_Evaluate(RotNet, batch_size=512)\n",
    "print(\"Training RotNet\")\n",
    "train_RotNet.train(x_train_rot,labels)\n",
    "print(\"Evaluating RotNet\")\n",
    "train_RotNet.evaluate(x_train_rot,labels)\n",
    "RotNet=train_RotNet.get_model()\n",
    "RotNet.save(\"NetWorkInNetwork_4_Blocks.model\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Now we move on to classification of the images themselves\")\n",
    "print(\"\\n\\n\")\n",
    "process_data.set_params(nr_of_classes=10)\n",
    "x_train,labels,x_test,labels_test=process_data.create_data(x_train,y_train,x_test,y_test,nomalize_data=True)\n",
    "\n",
    "print(\"Block 1 and non-linear classifier\")\n",
    "B1_non_lin=Transfer_learning(RotNet,blocks=1).transfer_algorithm()\n",
    "train_B1_non_lin=Train_and_Evaluate(B1_non_lin)\n",
    "train_B1_non_lin.set_step_decay(limit_1=20,limit_2=40,limit_3=45,limit_4=50)\n",
    "print(\"Training B1_non_lin\")\n",
    "train_B1_non_lin.train(x_train,labels)\n",
    "print(\"Evaluating B1_non_lin\")\n",
    "train_B1_non_lin.evaluate(x_test,labels_test)\n",
    "B1_non_lint=train_B1_non_lin.get_model()\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Block 2 and non-linear classifier\")\n",
    "B2_non_lin=Transfer_learning(RotNet,blocks=2).transfer_algorithm()\n",
    "train_B2_non_lin=Train_and_Evaluate(B2_non_lin)\n",
    "print(\"Training B2_non_lin\")\n",
    "train_B2_non_lin.set_step_decay(limit_1=20,limit_2=40,limit_3=45,limit_4=50)\n",
    "train_B2_non_lin.train(x_train,labels)\n",
    "print(\"Evaluating B2_non_lin\")\n",
    "train_B2_non_lin.evaluate(x_test,labels_test)\n",
    "B2_non_lin=train_B2_non_lin.get_model()\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Block 2 and non-linear classifier\")\n",
    "B3_non_lin=Transfer_learning(RotNet,blocks=3).transfer_algorithm()\n",
    "train_B3_non_lin=Train_and_Evaluate(B3_non_lin)\n",
    "print(\"Training B3_non_lin\")\n",
    "train_B3_non_lin.set_step_decay(limit_1=20,limit_2=40,limit_3=45,limit_4=50)\n",
    "train_B3_non_lin.train(x_train,labels)\n",
    "print(\"Evaluating B3_non_lin\")\n",
    "train_B3_non_lin.evaluate(x_test,labels_test)\n",
    "B3_non_lin=train_B3_non_lin.get_model()\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Block 2 and non-linear classifier\")\n",
    "B4_non_lin=Transfer_learning(RotNet,blocks=4).transfer_algorithm()\n",
    "train_B4_non_lin=Train_and_Evaluate(B4_non_lin)\n",
    "print(\"Training B4_non_lin\")\n",
    "train_B4_non_lin.set_step_decay(limit_1=20,limit_2=40,limit_3=45,limit_4=50)\n",
    "train_B4_non_lin.train(x_train,labels)\n",
    "print(\"Evaluating B4_non_lin\")\n",
    "train_B4_non_lin.evaluate(x_test,labels_test)\n",
    "B4_non_lin=train_B4_non_lin.get_model()\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Block 2 and RotNet+conv classifier\")\n",
    "B2_conv=Transfer_learning(RotNet,blocks=2).transfer_algorithm()\n",
    "train_B2_conv=Train_and_Evaluate(B2_conv)\n",
    "print(\"Training B2_conv\")\n",
    "train_B2_conv.set_step_decay(limit_1=35,limit_2=40,limit_3=85)\n",
    "train_B2_conv.train(x_train,labels)\n",
    "print(\"Evaluating B2_conv\")\n",
    "train_B2_conv.evaluate(x_test,labels_test)\n",
    "B2_conv=train_B2_conv.get_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
