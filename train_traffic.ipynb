{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def categorical_loss(F,Y):\n",
    "    return -tf.math.reduce_mean(tf.math.log(tf.reduce_sum(F*Y,1)),0)\n",
    "\n",
    "model=tf.keras.models.load_model(\"NetWorkInNetwork_4_Blocks.model\", custom_objects={'loss_kev': categorical_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.Sequential()\n",
    "\n",
    "for i in range(22):\n",
    "    layer = model.get_layer(index=i)\n",
    "    layer.trainable = False\n",
    "    new_model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 192)       14400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 32, 32, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 32, 32, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 160)       30720     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 32, 32, 160)       640       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 32, 32, 160)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 96)        15360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 32, 32, 96)        384       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 34, 34, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 192)       460800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16, 16, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 192)       36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16, 16, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 192)       36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16, 16, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 18, 18, 192)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 8, 8, 192)         0         \n",
      "=================================================================\n",
      "Total params: 599,104\n",
      "Trainable params: 0\n",
      "Non-trainable params: 599,104\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "my_init_4=tf.keras.initializers.RandomNormal(mean=0.0, stddev=np.math.sqrt(2.0/192))\n",
    "my_init_5=tf.keras.initializers.RandomNormal(mean=0.0, stddev=np.math.sqrt(2.0/(3*3*192)))\n",
    "my_init_dense=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='uniform')\n",
    "\n",
    "new_model.add(tf.keras.layers.Conv2D(192,kernel_size=3,strides=[1,1], kernel_initializer=my_init_5,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same', name='conv1'))\n",
    "new_model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05, name='bn1'))\n",
    "new_model.add(tf.keras.layers.ReLU(name='relu1'))\n",
    "new_model.add(tf.keras.layers.Conv2D(192,kernel_size=1,strides=[1,1], kernel_initializer=my_init_4,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same', name='conv2'))\n",
    "new_model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05, name='bn2'))\n",
    "new_model.add(tf.keras.layers.ReLU(name='relu2'))\n",
    "new_model.add(tf.keras.layers.Conv2D(192,kernel_size=1,strides=[1,1], kernel_initializer=my_init_4,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same', name='conv3'))\n",
    "new_model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05, name='bn3'))\n",
    "new_model.add(tf.keras.layers.ReLU(name='relu3'))\n",
    "\n",
    "new_model.add(tf.keras.layers.Flatten())\n",
    "new_model.add(tf.keras.layers.Dense(200, activation=None,kernel_initializer=my_init_dense, kernel_regularizer=tf.keras.regularizers.l2(5e-4),use_bias=False))\n",
    "new_model.add(tf.keras.layers.BatchNormalization(beta_initializer='zeros', gamma_initializer='ones', name='bn4'))\n",
    "new_model.add(tf.keras.layers.ReLU(name='relu4'))\n",
    "new_model.add(tf.keras.layers.Dense(200, activation=None,kernel_initializer=my_init_dense, kernel_regularizer=tf.keras.regularizers.l2(5e-4),use_bias=False))\n",
    "new_model.add(tf.keras.layers.BatchNormalization(beta_initializer='zeros', gamma_initializer='ones', name='bn5'))\n",
    "new_model.add(tf.keras.layers.ReLU(name='relu5'))\n",
    "new_model.add(tf.keras.layers.Dense(4, activation=None,kernel_initializer=my_init_dense, kernel_regularizer=tf.keras.regularizers.l2(5e-4), bias_initializer='zeros',bias_regularizer=tf.keras.regularizers.l2(5e-4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 192)       14400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 32, 32, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 32, 32, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 160)       30720     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 32, 32, 160)       640       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 32, 32, 160)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 96)        15360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 32, 32, 96)        384       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 34, 34, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 192)       460800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16, 16, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 192)       36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16, 16, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 192)       36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16, 16, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 18, 18, 192)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 8, 8, 192)         331776    \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalizationV1)   (None, 8, 8, 192)         768       \n",
      "_________________________________________________________________\n",
      "relu1 (ReLU)                 (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 8, 8, 192)         36864     \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalizationV1)   (None, 8, 8, 192)         768       \n",
      "_________________________________________________________________\n",
      "relu2 (ReLU)                 (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 8, 8, 192)         36864     \n",
      "_________________________________________________________________\n",
      "bn3 (BatchNormalizationV1)   (None, 8, 8, 192)         768       \n",
      "_________________________________________________________________\n",
      "relu3 (ReLU)                 (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               2457600   \n",
      "_________________________________________________________________\n",
      "bn4 (BatchNormalizationV1)   (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "relu4 (ReLU)                 (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               40000     \n",
      "_________________________________________________________________\n",
      "bn5 (BatchNormalizationV1)   (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "relu5 (ReLU)                 (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 804       \n",
      "=================================================================\n",
      "Total params: 3,506,916\n",
      "Trainable params: 2,905,860\n",
      "Non-trainable params: 601,056\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3639\n",
      "Number of training examples = 3093\n",
      "Number of testing examples = 546\n",
      "Image data shape = (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "f = open('TrafficDataset/coordinates.txt', 'r')\n",
    "lines=f.readlines()\n",
    "\n",
    "NUM_TEST = round(0.15*len(lines)) # 85% train, 15% test\n",
    "\n",
    "x_train = np.zeros((len(lines)-NUM_TEST, 32, 32, 3))\n",
    "y_train = np.zeros((len(lines)-NUM_TEST, 4))\n",
    "x_test = np.zeros((NUM_TEST, 32, 32, 3))\n",
    "y_test = np.zeros((NUM_TEST, 4))\n",
    "\n",
    "for i in range(len(lines)-NUM_TEST):\n",
    "    line = lines[i]\n",
    "    coords = [int(j) for j in line.split(';')[1:]]\n",
    "    img_path = 'TrafficDataset/'+str(i).zfill(6)+'.ppm'\n",
    "    img = cv2.imread(img_path)\n",
    "    x_train[i] = img\n",
    "    y_train[i,:] = coords\n",
    "\n",
    "for i in range(NUM_TEST):\n",
    "    line = lines[len(lines)-NUM_TEST+i]\n",
    "    coords = [int(j) for j in line.split(';')[1:]]\n",
    "    img_path = 'TrafficDataset/'+str(len(lines)-NUM_TEST+i).zfill(6)+'.ppm'\n",
    "    img = cv2.imread(img_path)\n",
    "    x_test[i] = img\n",
    "    y_test[i,:] = coords\n",
    "\n",
    "# Number of training examples\n",
    "n_train = len(x_train) \n",
    "\n",
    "# Number of testing examples.\n",
    "n_test = len(x_test) \n",
    "\n",
    "# What's the shape of the image\n",
    "image_shape = x_train[0].shape \n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "\n",
    "def normalize(img,img2):\n",
    "    img_array = np.asarray(img)\n",
    "    img_array2 = np.asarray(img2)\n",
    "    normalized = np.empty(np.shape(img_array))\n",
    "    normalized2 = np.empty(np.shape(img_array2))\n",
    "    for i in range(3):\n",
    "        normalized = (img_array - img_array[:,:,:,i].mean()) / (img_array[:,:,:,i].std())\n",
    "        normalized2 = (img_array2 - img_array[:,:,:,i].mean()) / (img_array[:,:,:,i].std())\n",
    "    return normalized,normalized2\n",
    "\n",
    "x_train,x_test=normalize(x_train,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "3056/3093 [============================>.] - ETA: 0s - loss: 76.6103 - acc: 0.4326lr: 0.05\n",
      "3093/3093 [==============================] - 6s 2ms/sample - loss: 76.5266 - acc: 0.4336\n",
      "Epoch 2/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 63.4238 - acc: 0.5016lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 63.4181 - acc: 0.5015\n",
      "Epoch 3/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 54.3729 - acc: 0.5573lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 54.3719 - acc: 0.5571\n",
      "Epoch 4/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 47.7831 - acc: 0.5557lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 47.8234 - acc: 0.5558\n",
      "Epoch 5/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 43.1817 - acc: 0.5651lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 43.2440 - acc: 0.5645\n",
      "Epoch 6/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 40.8382 - acc: 0.5687lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 40.8508 - acc: 0.5684\n",
      "Epoch 7/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 37.4140 - acc: 0.5703lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 37.3917 - acc: 0.5703\n",
      "Epoch 8/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 33.8723 - acc: 0.5654lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 33.9418 - acc: 0.5648\n",
      "Epoch 9/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 31.6072 - acc: 0.5916lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 31.6464 - acc: 0.5917\n",
      "Epoch 10/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 30.3213 - acc: 0.5949lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 30.3569 - acc: 0.5942\n",
      "Epoch 11/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 28.3804 - acc: 0.6098lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 28.3939 - acc: 0.6098\n",
      "Epoch 12/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 27.8576 - acc: 0.5962lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 27.8792 - acc: 0.5959\n",
      "Epoch 13/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 27.2528 - acc: 0.5884lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 27.2459 - acc: 0.5881\n",
      "Epoch 14/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 24.6928 - acc: 0.6159lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 24.6971 - acc: 0.6153\n",
      "Epoch 15/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 26.7506 - acc: 0.5929lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 26.7464 - acc: 0.5926\n",
      "Epoch 16/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 25.2616 - acc: 0.5939lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 25.2694 - acc: 0.5942\n",
      "Epoch 17/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 25.2172 - acc: 0.5975lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 25.2167 - acc: 0.5975\n",
      "Epoch 18/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 25.1410 - acc: 0.5923lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 25.1292 - acc: 0.5920\n",
      "Epoch 19/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 23.8253 - acc: 0.6182lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 23.8379 - acc: 0.6175\n",
      "Epoch 20/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 23.2691 - acc: 0.6211lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 23.2896 - acc: 0.6208\n",
      "Epoch 21/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 18.6099 - acc: 0.6580lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 18.6276 - acc: 0.6586\n",
      "Epoch 22/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 15.5987 - acc: 0.6891lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 15.6780 - acc: 0.6893\n",
      "Epoch 23/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 15.4446 - acc: 0.6911lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 15.4470 - acc: 0.6912\n",
      "Epoch 24/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 14.5317 - acc: 0.6927lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 14.5406 - acc: 0.6922\n",
      "Epoch 25/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 13.4799 - acc: 0.7166lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 13.4792 - acc: 0.7168\n",
      "Epoch 26/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 13.1663 - acc: 0.7150lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 13.1814 - acc: 0.7155\n",
      "Epoch 27/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 13.0670 - acc: 0.7361lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 13.0965 - acc: 0.7359\n",
      "Epoch 28/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 12.5568 - acc: 0.7351lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 12.5555 - acc: 0.7352\n",
      "Epoch 29/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 12.2077 - acc: 0.7296lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 12.2284 - acc: 0.7294\n",
      "Epoch 30/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 11.6342 - acc: 0.7432lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 11.6454 - acc: 0.7436\n",
      "Epoch 31/100\n",
      "3072/3093 [============================>.] - ETA: 0s - loss: 11.8173 - acc: 0.7269lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 11.8915 - acc: 0.7249\n",
      "Epoch 32/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 11.7682 - acc: 0.7286lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 11.7719 - acc: 0.7281\n",
      "Epoch 33/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 11.1692 - acc: 0.7464lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 11.1954 - acc: 0.7462\n",
      "Epoch 34/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 10.9281 - acc: 0.7277lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 10.9535 - acc: 0.7274\n",
      "Epoch 35/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 10.3030 - acc: 0.7383lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 10.3146 - acc: 0.7375\n",
      "Epoch 36/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 10.4408 - acc: 0.7387lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 10.4492 - acc: 0.7388\n",
      "Epoch 37/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 10.3830 - acc: 0.7390lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 10.3794 - acc: 0.7394\n",
      "Epoch 38/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 9.9246 - acc: 0.7484lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 9.9370 - acc: 0.7481\n",
      "Epoch 39/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 9.9932 - acc: 0.7526 lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 9.9965 - acc: 0.7523\n",
      "Epoch 40/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 9.5853 - acc: 0.7438lr: 0.002\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 9.5941 - acc: 0.7436\n",
      "Epoch 41/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 8.9583 - acc: 0.7795lr: 0.002\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 8.9570 - acc: 0.7795\n",
      "Epoch 42/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 8.6673 - acc: 0.7775lr: 0.002\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 8.6671 - acc: 0.7776\n",
      "Epoch 43/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 8.3107 - acc: 0.7811lr: 0.002\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 8.3102 - acc: 0.7814\n",
      "Epoch 44/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 8.2265 - acc: 0.7710lr: 0.002\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 8.2508 - acc: 0.7698\n",
      "Epoch 45/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 8.0674 - acc: 0.7804lr: 0.0004\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 8.0889 - acc: 0.7798\n",
      "Epoch 46/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.8038 - acc: 0.7740lr: 0.0004\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.8584 - acc: 0.7734\n",
      "Epoch 47/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.9257 - acc: 0.7970lr: 0.0004\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.9326 - acc: 0.7966\n",
      "Epoch 48/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.8935 - acc: 0.7963lr: 0.0004\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.9176 - acc: 0.7963\n",
      "Epoch 49/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.7166 - acc: 0.7973lr: 0.0004\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.7197 - acc: 0.7970\n",
      "Epoch 50/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.9018 - acc: 0.7824lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.9062 - acc: 0.7824\n",
      "Epoch 51/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.4295 - acc: 0.8076lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.4478 - acc: 0.8070\n",
      "Epoch 52/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.7756 - acc: 0.8054lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.7937 - acc: 0.8050\n",
      "Epoch 53/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5028 - acc: 0.7970lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5057 - acc: 0.7970\n",
      "Epoch 54/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5850 - acc: 0.7889lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.7312 - acc: 0.7879\n",
      "Epoch 55/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5312 - acc: 0.7950lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5532 - acc: 0.7950\n",
      "Epoch 56/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5922 - acc: 0.7918lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.6011 - acc: 0.7918\n",
      "Epoch 57/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.8174 - acc: 0.7989lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.8480 - acc: 0.7986\n",
      "Epoch 58/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.6946 - acc: 0.7999lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.7466 - acc: 0.7995\n",
      "Epoch 59/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.8667 - acc: 0.7885lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.8812 - acc: 0.7882\n",
      "Epoch 60/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.7402 - acc: 0.7947lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.7570 - acc: 0.7947\n",
      "Epoch 61/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.6965 - acc: 0.8057lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.7206 - acc: 0.8057\n",
      "Epoch 62/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5764 - acc: 0.8002lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5883 - acc: 0.7999\n",
      "Epoch 63/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.9518 - acc: 0.7937lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.9811 - acc: 0.7931\n",
      "Epoch 64/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.8763 - acc: 0.7882lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.8752 - acc: 0.7882\n",
      "Epoch 65/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.4451 - acc: 0.8057lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5111 - acc: 0.8057\n",
      "Epoch 66/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5390 - acc: 0.7882lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5473 - acc: 0.7882\n",
      "Epoch 67/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.6064 - acc: 0.7908lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.6044 - acc: 0.7905\n",
      "Epoch 68/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.7454 - acc: 0.7937lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.7544 - acc: 0.7941\n",
      "Epoch 69/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.3635 - acc: 0.7979lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.3653 - acc: 0.7973\n",
      "Epoch 70/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5738 - acc: 0.8002lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5800 - acc: 0.7999\n",
      "Epoch 71/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.7497 - acc: 0.7927lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.7564 - acc: 0.7928\n",
      "Epoch 72/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.7085 - acc: 0.7895lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.7363 - acc: 0.7889\n",
      "Epoch 73/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5202 - acc: 0.8063lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5765 - acc: 0.8060\n",
      "Epoch 74/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.3941 - acc: 0.8096lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.4018 - acc: 0.8092\n",
      "Epoch 75/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5003 - acc: 0.7973lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5004 - acc: 0.7973\n",
      "Epoch 76/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.6598 - acc: 0.7889lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.6588 - acc: 0.7889\n",
      "Epoch 77/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.6782 - acc: 0.7979lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.6767 - acc: 0.7976\n",
      "Epoch 78/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5027 - acc: 0.8093lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5180 - acc: 0.8086\n",
      "Epoch 79/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.3143 - acc: 0.8138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.3433 - acc: 0.8131\n",
      "Epoch 80/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5254 - acc: 0.7953lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5328 - acc: 0.7950\n",
      "Epoch 81/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5063 - acc: 0.8089lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5147 - acc: 0.8089\n",
      "Epoch 82/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.3934 - acc: 0.8015lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.3971 - acc: 0.8018\n",
      "Epoch 83/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5903 - acc: 0.8031lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.6215 - acc: 0.8025\n",
      "Epoch 84/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.4735 - acc: 0.8054lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5287 - acc: 0.8054\n",
      "Epoch 85/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.4673 - acc: 0.8034lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.4780 - acc: 0.8034\n",
      "Epoch 86/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.4454 - acc: 0.7947lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.4502 - acc: 0.7950\n",
      "Epoch 87/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.4562 - acc: 0.8008lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.4842 - acc: 0.8005\n",
      "Epoch 88/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.3089 - acc: 0.7999lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.3126 - acc: 0.8002\n",
      "Epoch 89/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5297 - acc: 0.8031lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5708 - acc: 0.8031\n",
      "Epoch 90/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.4251 - acc: 0.8012lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.4467 - acc: 0.8008\n",
      "Epoch 91/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.4096 - acc: 0.8089lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.4213 - acc: 0.8089\n",
      "Epoch 92/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.6203 - acc: 0.8012lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.6210 - acc: 0.8012\n",
      "Epoch 93/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.3760 - acc: 0.7934lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.3946 - acc: 0.7934\n",
      "Epoch 94/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.4485 - acc: 0.7983lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.4485 - acc: 0.7986\n",
      "Epoch 95/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.3943 - acc: 0.7976lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.4140 - acc: 0.7976\n",
      "Epoch 96/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.2620 - acc: 0.8115lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.3027 - acc: 0.8115\n",
      "Epoch 97/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.4653 - acc: 0.8025lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.4667 - acc: 0.8025\n",
      "Epoch 98/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.5167 - acc: 0.8112lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.5295 - acc: 0.8109\n",
      "Epoch 99/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.6843 - acc: 0.7944lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.7077 - acc: 0.7937\n",
      "Epoch 100/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 7.9333 - acc: 0.7876lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 7.9495 - acc:0.7884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f95810d4080>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "        print('lr:', step_decay(len(self.losses)))\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.05\n",
    "    if epoch<20:\n",
    "        lrate = initial_lrate\n",
    "    elif epoch<40:\n",
    "        lrate = initial_lrate/5\n",
    "    elif epoch<45:\n",
    "        lrate = initial_lrate/25\n",
    "    elif epoch<50:\n",
    "        lrate = initial_lrate/(25*5)\n",
    "    else:\n",
    "        lrate = initial_lrate/(25*25)\n",
    "    return lrate\n",
    "\n",
    "loss_history = LossHistory()\n",
    "lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "callbacks_list = [loss_history, lrate]\n",
    "\n",
    "my_optimizer=tf.keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "\n",
    "new_model.compile(optimizer=my_optimizer,loss='mean_squared_error',metrics=['accuracy'])\n",
    "new_model.fit(x_train,y_train, batch_size=16,epochs=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 1s 1ms/sample - loss: 17.5466 - acc: 0.6991\n",
      "17.546561372223625 0.69905216\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc=new_model.evaluate(x_test,y_test)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save(\"traffic_sign.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
