{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3639\n",
      "Number of training examples = 3093\n",
      "Number of testing examples = 546\n",
      "Image data shape = (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "f = open('TrafficDataset/coordinates.txt', 'r')\n",
    "lines=f.readlines()\n",
    "\n",
    "NUM_TEST = round(0.15*len(lines)) # 85% train, 15% test\n",
    "\n",
    "x_train = np.zeros((len(lines)-NUM_TEST, 32, 32, 3))\n",
    "y_train = np.zeros((len(lines)-NUM_TEST, 4))\n",
    "x_test = np.zeros((NUM_TEST, 32, 32, 3))\n",
    "y_test = np.zeros((NUM_TEST, 4))\n",
    "\n",
    "for i in range(len(lines)-NUM_TEST):\n",
    "    line = lines[i]\n",
    "    coords = [int(j) for j in line.split(';')[1:]]\n",
    "    img_path = 'TrafficDataset/'+str(i).zfill(6)+'.ppm'\n",
    "    img = cv2.imread(img_path)\n",
    "    x_train[i] = img\n",
    "    y_train[i,:] = coords\n",
    "\n",
    "for i in range(NUM_TEST):\n",
    "    line = lines[len(lines)-NUM_TEST+i]\n",
    "    coords = [int(j) for j in line.split(';')[1:]]\n",
    "    img_path = 'TrafficDataset/'+str(len(lines)-NUM_TEST+i).zfill(6)+'.ppm'\n",
    "    img = cv2.imread(img_path)\n",
    "    x_test[i] = img\n",
    "    y_test[i,:] = coords\n",
    "\n",
    "# Number of training examples\n",
    "n_train = len(x_train) \n",
    "\n",
    "# Number of testing examples.\n",
    "n_test = len(x_test) \n",
    "\n",
    "# What's the shape of the image\n",
    "image_shape = x_train[0].shape \n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "\n",
    "def normalize(img,img2):\n",
    "    img_array = np.asarray(img)\n",
    "    img_array2 = np.asarray(img2)\n",
    "    normalized = np.empty(np.shape(img_array))\n",
    "    normalized2 = np.empty(np.shape(img_array2))\n",
    "    for i in range(3):\n",
    "        normalized = (img_array - img_array[:,:,:,i].mean()) / (img_array[:,:,:,i].std())\n",
    "        normalized2 = (img_array2 - img_array[:,:,:,i].mean()) / (img_array[:,:,:,i].std())\n",
    "    return normalized,normalized2\n",
    "\n",
    "x_train,x_test=normalize(x_train,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "3072/3093 [============================>.] - ETA: 0s - loss: 108.3355 - acc: 0.2259lr: 0.05\n",
      "3093/3093 [==============================] - 6s 2ms/sample - loss: 108.4476 - acc: 0.2244\n",
      "Epoch 2/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 98.2300 - acc: 0.2290lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 98.2036 - acc: 0.2292\n",
      "Epoch 3/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 89.4868 - acc: 0.2432lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 89.4787 - acc: 0.2431\n",
      "Epoch 4/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 82.2655 - acc: 0.2448lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 82.2202 - acc: 0.2444\n",
      "Epoch 5/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 76.0474 - acc: 0.2299lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 76.0477 - acc: 0.2299\n",
      "Epoch 6/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 71.0949 - acc: 0.2170lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 71.0847 - acc: 0.2176\n",
      "Epoch 7/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 67.0592 - acc: 0.2302lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 67.0521 - acc: 0.2299\n",
      "Epoch 8/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 63.6735 - acc: 0.2325lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 63.6429 - acc: 0.2325\n",
      "Epoch 9/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 60.8547 - acc: 0.2461lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 60.8398 - acc: 0.2457\n",
      "Epoch 10/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 58.6392 - acc: 0.2471lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 58.6499 - acc: 0.2467\n",
      "Epoch 11/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 56.6132 - acc: 0.2273lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 56.6491 - acc: 0.2276\n",
      "Epoch 12/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 55.0756 - acc: 0.2370lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 55.0442 - acc: 0.2367\n",
      "Epoch 13/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 53.8793 - acc: 0.2655lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 53.8536 - acc: 0.2664\n",
      "Epoch 14/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 52.7689 - acc: 0.2215lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 52.7626 - acc: 0.2221\n",
      "Epoch 15/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 52.0085 - acc: 0.2413lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 51.9659 - acc: 0.2415\n",
      "Epoch 16/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 51.2456 - acc: 0.2286lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 51.2598 - acc: 0.2286\n",
      "Epoch 17/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 50.5281 - acc: 0.2364lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 50.6172 - acc: 0.2360\n",
      "Epoch 18/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 50.2506 - acc: 0.2406lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 50.2259 - acc: 0.2409\n",
      "Epoch 19/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 49.6943 - acc: 0.2202lr: 0.05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 49.6747 - acc: 0.2199\n",
      "Epoch 20/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 49.4253 - acc: 0.2234lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 49.4042 - acc: 0.2237\n",
      "Epoch 21/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 49.1378 - acc: 0.2516lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 49.0822 - acc: 0.2522\n",
      "Epoch 22/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.9779 - acc: 0.2688lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.9577 - acc: 0.2690\n",
      "Epoch 23/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.9600 - acc: 0.2685lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.9387 - acc: 0.2680\n",
      "Epoch 24/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.9141 - acc: 0.2639lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.8744 - acc: 0.2641\n",
      "Epoch 25/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.8080 - acc: 0.2587lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.8428 - acc: 0.2586\n",
      "Epoch 26/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.7817 - acc: 0.2778lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.7612 - acc: 0.2777\n",
      "Epoch 27/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.7195 - acc: 0.2506lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.7522 - acc: 0.2509\n",
      "Epoch 28/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.7265 - acc: 0.2549lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.7292 - acc: 0.2548\n",
      "Epoch 29/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.6705 - acc: 0.2707lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.6444 - acc: 0.2703\n",
      "Epoch 30/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.6342 - acc: 0.2672lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.6236 - acc: 0.2671\n",
      "Epoch 31/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.6343 - acc: 0.2474lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.6225 - acc: 0.2477\n",
      "Epoch 32/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.5508 - acc: 0.2348lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.5693 - acc: 0.2347\n",
      "Epoch 33/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.5667 - acc: 0.2532lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.5517 - acc: 0.2535\n",
      "Epoch 34/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.4905 - acc: 0.2937lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.4663 - acc: 0.2939\n",
      "Epoch 35/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.4930 - acc: 0.2623lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.4595 - acc: 0.2619\n",
      "Epoch 36/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.4583 - acc: 0.2759lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.4343 - acc: 0.2755\n",
      "Epoch 37/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.4061 - acc: 0.2613lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.3930 - acc: 0.2619\n",
      "Epoch 38/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.3937 - acc: 0.2804lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.3991 - acc: 0.2803\n",
      "Epoch 39/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.3585 - acc: 0.2390lr: 0.01\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.3262 - acc: 0.2392\n",
      "Epoch 40/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.3590 - acc: 0.2652lr: 0.002\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.3248 - acc: 0.2651\n",
      "Epoch 41/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2877 - acc: 0.3093lr: 0.002\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.2474 - acc: 0.3104\n",
      "Epoch 42/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2206 - acc: 0.3131lr: 0.002\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.2409 - acc: 0.3136\n",
      "Epoch 43/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2296 - acc: 0.3135lr: 0.002\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.2315 - acc: 0.3136\n",
      "Epoch 44/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2118 - acc: 0.3135lr: 0.002\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.2303 - acc: 0.3136\n",
      "Epoch 45/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2626 - acc: 0.3141lr: 0.0004\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.2250 - acc: 0.3136\n",
      "Epoch 46/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1946 - acc: 0.3138lr: 0.0004\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.2090 - acc: 0.3136\n",
      "Epoch 47/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1730 - acc: 0.3138lr: 0.0004\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.2076 - acc: 0.3136\n",
      "Epoch 48/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2436 - acc: 0.3138lr: 0.0004\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.2055 - acc: 0.3136\n",
      "Epoch 49/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2531 - acc: 0.3135lr: 0.0004\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.2050 - acc: 0.3136\n",
      "Epoch 50/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2087 - acc: 0.3135lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.2027 - acc: 0.3136\n",
      "Epoch 51/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1678 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1986 - acc: 0.3136\n",
      "Epoch 52/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2252 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1983 - acc: 0.3136\n",
      "Epoch 53/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1837 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1983 - acc: 0.3136\n",
      "Epoch 54/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2384 - acc: 0.3135lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1979 - acc: 0.3136\n",
      "Epoch 55/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2062 - acc: 0.3141lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1975 - acc: 0.3136\n",
      "Epoch 56/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2129 - acc: 0.3131lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1977 - acc: 0.3136\n",
      "Epoch 57/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2045 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1971 - acc: 0.3136\n",
      "Epoch 58/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1906 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1974 - acc: 0.3136\n",
      "Epoch 59/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1957 - acc: 0.3135lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1968 - acc: 0.3136\n",
      "Epoch 60/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1614 - acc: 0.3131lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1970 - acc: 0.3136\n",
      "Epoch 61/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1986 - acc: 0.3131lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1964 - acc: 0.3136\n",
      "Epoch 62/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1060 - acc: 0.3128lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1966 - acc: 0.3136\n",
      "Epoch 63/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2369 - acc: 0.3128lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1962 - acc: 0.3136\n",
      "Epoch 64/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1872 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1962 - acc: 0.3136\n",
      "Epoch 65/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2102 - acc: 0.3135lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1958 - acc: 0.3136\n",
      "Epoch 66/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2051 - acc: 0.3135lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1956 - acc: 0.3136\n",
      "Epoch 67/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2377 - acc: 0.3131lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1954 - acc: 0.3136\n",
      "Epoch 68/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2499 - acc: 0.3135lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1954 - acc: 0.3136\n",
      "Epoch 69/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2232 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1950 - acc: 0.3136\n",
      "Epoch 70/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1837 - acc: 0.3135lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1948 - acc: 0.3136\n",
      "Epoch 71/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1535 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1948 - acc: 0.3136\n",
      "Epoch 72/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2337 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1948 - acc: 0.3136\n",
      "Epoch 73/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1630 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1942 - acc: 0.3136\n",
      "Epoch 74/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2102 - acc: 0.3141lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1942 - acc: 0.3136\n",
      "Epoch 75/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1642 - acc: 0.3135lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1943 - acc: 0.3136\n",
      "Epoch 76/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2304 - acc: 0.3128lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1937 - acc: 0.3136\n",
      "Epoch 77/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2289 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1934 - acc: 0.3136\n",
      "Epoch 78/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2347 - acc: 0.3135lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1932 - acc: 0.3136\n",
      "Epoch 79/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2333 - acc: 0.3131lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1935 - acc: 0.3136\n",
      "Epoch 80/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1792 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1930 - acc: 0.3136\n",
      "Epoch 81/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2149 - acc: 0.3131lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1928 - acc: 0.3136\n",
      "Epoch 82/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2134 - acc: 0.3135lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1927 - acc: 0.3136\n",
      "Epoch 83/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2002 - acc: 0.3141lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1923 - acc: 0.3136\n",
      "Epoch 84/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1374 - acc: 0.3131lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1924 - acc: 0.3136\n",
      "Epoch 85/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1981 - acc: 0.3131lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1922 - acc: 0.3136\n",
      "Epoch 86/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2231 - acc: 0.3141lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1920 - acc: 0.3136\n",
      "Epoch 87/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1797 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1915 - acc: 0.3136\n",
      "Epoch 88/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2145 - acc: 0.3131lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1915 - acc: 0.3136\n",
      "Epoch 89/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2237 - acc: 0.3131lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1913 - acc: 0.3136\n",
      "Epoch 90/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1977 - acc: 0.3141lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1910 - acc: 0.3136\n",
      "Epoch 91/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2245 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1909 - acc: 0.3136\n",
      "Epoch 92/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1805 - acc: 0.3141lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1907 - acc: 0.3136\n",
      "Epoch 93/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2113 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1903 - acc: 0.3136\n",
      "Epoch 94/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1968 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1903 - acc: 0.3136\n",
      "Epoch 95/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2233 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1904 - acc: 0.3136\n",
      "Epoch 96/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2260 - acc: 0.3135lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1902 - acc: 0.3136\n",
      "Epoch 97/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1971 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1897 - acc: 0.3136\n",
      "Epoch 98/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2166 - acc: 0.3135lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1896 - acc: 0.3136\n",
      "Epoch 99/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.2173 - acc: 0.3138lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1898 - acc: 0.3136\n",
      "Epoch 100/100\n",
      "3088/3093 [============================>.] - ETA: 0s - loss: 48.1568 - acc: 0.3128lr: 8e-05\n",
      "3093/3093 [==============================] - 4s 1ms/sample - loss: 48.1891 - acc: 0.3136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f47403df390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "        print('lr:', step_decay(len(self.losses)))\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.05\n",
    "    if epoch<20:\n",
    "        lrate = initial_lrate\n",
    "    elif epoch<40:\n",
    "        lrate = initial_lrate/5\n",
    "    elif epoch<45:\n",
    "        lrate = initial_lrate/25\n",
    "    elif epoch<50:\n",
    "        lrate = initial_lrate/(25*5)\n",
    "    else:\n",
    "        lrate = initial_lrate/(25*25)\n",
    "    return lrate\n",
    "\n",
    "loss_history = LossHistory()\n",
    "lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "callbacks_list = [loss_history, lrate]\n",
    "\n",
    "model=tf.keras.models.Sequential()\n",
    "\n",
    "my_init_1=tf.keras.initializers.RandomNormal(mean=0.0, stddev=tf.math.sqrt(2.0/(5*5*192)))\n",
    "my_init_2=tf.keras.initializers.RandomNormal(mean=0.0, stddev=tf.math.sqrt(2.0/160))\n",
    "my_init_3=tf.keras.initializers.RandomNormal(mean=0.0, stddev=tf.math.sqrt(2.0/96))\n",
    "my_init_4=tf.keras.initializers.RandomNormal(mean=0.0, stddev=tf.math.sqrt(2.0/192))\n",
    "my_init_5=tf.keras.initializers.RandomNormal(mean=0.0, stddev=tf.math.sqrt(2.0/(3*3*192)))\n",
    "my_init_dense=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='uniform')\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=5,strides=[1,1],kernel_initializer=my_init_1,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False , input_shape=(32,32,3), padding='same',data_format='channels_last',trainable = False))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05,trainable = False))\n",
    "model.add(tf.keras.layers.ReLU(trainable = False))\n",
    "model.add(tf.keras.layers.Conv2D(160,kernel_size=1,strides=[1,1], kernel_initializer=my_init_2, kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same',trainable = False))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05,trainable = False))\n",
    "model.add(tf.keras.layers.ReLU(trainable = False))\n",
    "model.add(tf.keras.layers.Conv2D(96,kernel_size=1,strides=[1,1], kernel_initializer=my_init_3,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same',trainable = False))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05,trainable = False))\n",
    "model.add(tf.keras.layers.ReLU(trainable = False))\n",
    "model.add(tf.keras.layers.ZeroPadding2D(padding=(1, 1),trainable = False))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2,2),trainable = False))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=5,strides=[1,1], kernel_initializer=my_init_1,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same',trainable = False))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05,trainable = False))\n",
    "model.add(tf.keras.layers.ReLU(trainable = False))\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=1,strides=[1,1], kernel_initializer=my_init_4,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same',trainable = False))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05,trainable = False))\n",
    "model.add(tf.keras.layers.ReLU(trainable = False))\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=1,strides=[1,1], kernel_initializer=my_init_4,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same',trainable = False))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05,trainable = False))\n",
    "model.add(tf.keras.layers.ReLU(trainable = False))\n",
    "model.add(tf.keras.layers.ZeroPadding2D(padding=(1, 1),trainable = False))\n",
    "model.add(tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(2,2),trainable = False))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=3,strides=[1,1], kernel_initializer=my_init_5,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same', name='conv1'))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05, name='bn1'))\n",
    "model.add(tf.keras.layers.ReLU(name='relu1'))\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=1,strides=[1,1], kernel_initializer=my_init_4,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same', name='conv2'))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05, name='bn2'))\n",
    "model.add(tf.keras.layers.ReLU(name='relu2'))\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=1,strides=[1,1], kernel_initializer=my_init_4,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same', name='conv3'))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05, name='bn3'))\n",
    "model.add(tf.keras.layers.ReLU(name='relu3'))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(200, activation=None,kernel_initializer=my_init_dense, kernel_regularizer=tf.keras.regularizers.l2(5e-4),use_bias=False))\n",
    "model.add(tf.keras.layers.BatchNormalization(beta_initializer='zeros', gamma_initializer='ones', name='bn4'))\n",
    "model.add(tf.keras.layers.ReLU(name='relu4'))\n",
    "model.add(tf.keras.layers.Dense(200, activation=None,kernel_initializer=my_init_dense, kernel_regularizer=tf.keras.regularizers.l2(5e-4),use_bias=False))\n",
    "model.add(tf.keras.layers.BatchNormalization(beta_initializer='zeros', gamma_initializer='ones', name='bn5'))\n",
    "model.add(tf.keras.layers.ReLU(name='relu5'))\n",
    "model.add(tf.keras.layers.Dense(4, activation=None,kernel_initializer=my_init_dense, kernel_regularizer=tf.keras.regularizers.l2(5e-4), bias_initializer='zeros',bias_regularizer=tf.keras.regularizers.l2(5e-4)))\n",
    "\n",
    "my_optimizer=tf.keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=my_optimizer,loss='mean_squared_error',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train, batch_size=16, epochs=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 1s 1ms/sample - loss: 44.7982 - acc: 0.3370\n",
      "44.79820043263418 0.33699635\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc=model.evaluate(x_test,y_test)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 123.18091941218806 0.22511849"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
