{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 50000\n",
      "Number of testing examples = 10000\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 10\n",
      "<class 'numpy.ndarray'>\n",
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Number of training examples\n",
    "n_train = len(x_train) \n",
    "\n",
    "# Number of testing examples.\n",
    "n_test = len(x_test) \n",
    "\n",
    "# Shape of the images\n",
    "image_shape = x_train[0].shape \n",
    "\n",
    "# How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from PIL import Image\n",
    "\n",
    "lr = tf.Variable(0.1) # learning rate\n",
    "MOMENTUM = 0.9\n",
    "DECAY = 5e-4 \n",
    "K=10\n",
    "\n",
    "num_examples = len(x_train)\n",
    "num_test = len(x_test)\n",
    "x = tf.placeholder(tf.float32, shape=(None,)+image_shape)\n",
    "y = tf.placeholder(tf.int32)\n",
    "Y = tf.placeholder(tf.float32, shape=(None,K))\n",
    "\n",
    "def normalize(img,img2):\n",
    "    img_array = np.asarray(img)\n",
    "    img_array2 = np.asarray(img2)\n",
    "    normalized = np.empty(np.shape(img_array))\n",
    "    normalized2 = np.empty(np.shape(img_array2))\n",
    "    for i in range(3):\n",
    "        normalized = (img_array - img_array[:,:,:,i].mean()) / (img_array[:,:,:,i].std())\n",
    "        normalized2 = (img_array2 - img_array[:,:,:,i].mean()) / (img_array[:,:,:,i].std())\n",
    "    return normalized,normalized2\n",
    "\n",
    "x_train,x_test=normalize(x_train,x_test)\n",
    "\n",
    "def get_one_hot(y,K):\n",
    "    one_hot=np.zeros((np.shape(y)[0],K))\n",
    "    for j in range(np.shape(y)[0]):\n",
    "        one_hot[j,y[j]]=1\n",
    "    return one_hot\n",
    "\n",
    "def categorical_loss(F,Y):\n",
    "    return -tf.math.reduce_mean(tf.math.log(tf.reduce_sum(F*Y,1)),0)\n",
    "\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "        print('lr:', step_decay(len(self.losses)))\n",
    "        \n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    if epoch<35:\n",
    "        lrate = initial_lrate\n",
    "    elif epoch<70:\n",
    "        lrate = initial_lrate/5\n",
    "    elif epoch<85:\n",
    "        lrate = initial_lrate/25\n",
    "    else:\n",
    "        lrate = initial_lrate/(25*5)\n",
    "    return lrate\n",
    "\n",
    "loss_history = LossHistory()\n",
    "lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "callbacks_list = [loss_history, lrate]\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess: \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    labels=get_one_hot(y_train,K)\n",
    "    labels_test=get_one_hot(y_test,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.6610 - acc: 0.4998lr: 0.1\n",
      "50000/50000 [==============================] - 24s 489us/sample - loss: 2.6605 - acc: 0.4999\n",
      "Epoch 2/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.2802 - acc: 0.5875lr: 0.1\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 2.2803 - acc: 0.5875\n",
      "Epoch 3/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.1460 - acc: 0.6184lr: 0.1\n",
      "50000/50000 [==============================] - 23s 457us/sample - loss: 2.1456 - acc: 0.6184\n",
      "Epoch 4/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0817 - acc: 0.6352lr: 0.1\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 2.0816 - acc: 0.6352\n",
      "Epoch 5/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0503 - acc: 0.6483lr: 0.1\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 2.0502 - acc: 0.6483\n",
      "Epoch 6/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0284 - acc: 0.6600lr: 0.1\n",
      "50000/50000 [==============================] - 23s 458us/sample - loss: 2.0284 - acc: 0.6600\n",
      "Epoch 7/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0209 - acc: 0.6662lr: 0.1\n",
      "50000/50000 [==============================] - 23s 457us/sample - loss: 2.0209 - acc: 0.6662\n",
      "Epoch 8/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0140 - acc: 0.6741lr: 0.1\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 2.0140 - acc: 0.6740\n",
      "Epoch 9/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0130 - acc: 0.6782lr: 0.1\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 2.0129 - acc: 0.6782\n",
      "Epoch 10/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0049 - acc: 0.6848lr: 0.1\n",
      "50000/50000 [==============================] - 23s 457us/sample - loss: 2.0051 - acc: 0.6849\n",
      "Epoch 11/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0060 - acc: 0.6897lr: 0.1\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 2.0066 - acc: 0.6895\n",
      "Epoch 12/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0050 - acc: 0.6953lr: 0.1\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 2.0051 - acc: 0.6953\n",
      "Epoch 13/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0075 - acc: 0.6922lr: 0.1\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 2.0074 - acc: 0.6923\n",
      "Epoch 14/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0092 - acc: 0.6979lr: 0.1\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 2.0092 - acc: 0.6979\n",
      "Epoch 15/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0065 - acc: 0.7024lr: 0.1\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 2.0065 - acc: 0.7024\n",
      "Epoch 16/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0066 - acc: 0.7005lr: 0.1\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 2.0066 - acc: 0.7005\n",
      "Epoch 17/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0035 - acc: 0.7039lr: 0.1\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 2.0037 - acc: 0.7039\n",
      "Epoch 18/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0070 - acc: 0.7046lr: 0.1\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 2.0071 - acc: 0.7046\n",
      "Epoch 19/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0042 - acc: 0.7082lr: 0.1\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 2.0042 - acc: 0.7082\n",
      "Epoch 20/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0083 - acc: 0.7090lr: 0.1\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 2.0082 - acc: 0.7090\n",
      "Epoch 21/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0075 - acc: 0.7104lr: 0.1\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 2.0074 - acc: 0.7104\n",
      "Epoch 22/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0023 - acc: 0.7105lr: 0.1\n",
      "50000/50000 [==============================] - 23s 457us/sample - loss: 2.0024 - acc: 0.7104\n",
      "Epoch 23/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.9997 - acc: 0.7132lr: 0.1\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.9994 - acc: 0.7134\n",
      "Epoch 24/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0066 - acc: 0.7100lr: 0.1\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 2.0063 - acc: 0.7102\n",
      "Epoch 25/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0048 - acc: 0.7113lr: 0.1\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 2.0046 - acc: 0.7114\n",
      "Epoch 26/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0050 - acc: 0.7136lr: 0.1\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 2.0048 - acc: 0.7136\n",
      "Epoch 27/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0041 - acc: 0.7135lr: 0.1\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 2.0038 - acc: 0.7136\n",
      "Epoch 28/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0028 - acc: 0.7150lr: 0.1\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 2.0030 - acc: 0.7149\n",
      "Epoch 29/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0025 - acc: 0.7185lr: 0.1\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 2.0025 - acc: 0.7186\n",
      "Epoch 30/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0058 - acc: 0.7139lr: 0.1\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 2.0060 - acc: 0.7139\n",
      "Epoch 31/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0016 - acc: 0.7188lr: 0.1\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 2.0017 - acc: 0.7188\n",
      "Epoch 32/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0023 - acc: 0.7211lr: 0.1\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 2.0024 - acc: 0.7211\n",
      "Epoch 33/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.9993 - acc: 0.7187lr: 0.1\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.9991 - acc: 0.7188\n",
      "Epoch 34/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0070 - acc: 0.7153lr: 0.1\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 2.0069 - acc: 0.7153\n",
      "Epoch 35/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.0053 - acc: 0.7170lr: 0.02\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 2.0056 - acc: 0.7168\n",
      "Epoch 36/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.7487 - acc: 0.8102lr: 0.02\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.7486 - acc: 0.8104\n",
      "Epoch 37/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.5982 - acc: 0.8497lr: 0.02\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.5982 - acc: 0.8497\n",
      "Epoch 38/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.5320 - acc: 0.8650lr: 0.02\n",
      "50000/50000 [==============================] - 23s 457us/sample - loss: 1.5323 - acc: 0.8649\n",
      "Epoch 39/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4881 - acc: 0.8762lr: 0.02\n",
      "50000/50000 [==============================] - 23s 453us/sample - loss: 1.4883 - acc: 0.8761\n",
      "Epoch 40/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4718 - acc: 0.8783lr: 0.02\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.4720 - acc: 0.8782\n",
      "Epoch 41/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4624 - acc: 0.8799lr: 0.02\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.4627 - acc: 0.8798\n",
      "Epoch 42/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4541 - acc: 0.8827lr: 0.02\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.4542 - acc: 0.8826\n",
      "Epoch 43/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4480 - acc: 0.8862lr: 0.02\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.4479 - acc: 0.8862\n",
      "Epoch 44/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4502 - acc: 0.8883lr: 0.02\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.4503 - acc: 0.8883\n",
      "Epoch 45/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4366 - acc: 0.8941lr: 0.02\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.4367 - acc: 0.8940\n",
      "Epoch 46/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4391 - acc: 0.8964lr: 0.02\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.4391 - acc: 0.8964\n",
      "Epoch 47/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4316 - acc: 0.8993lr: 0.02\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.4319 - acc: 0.8992\n",
      "Epoch 48/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4301 - acc: 0.9014lr: 0.02\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.4303 - acc: 0.9013\n",
      "Epoch 49/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4312 - acc: 0.9035lr: 0.02\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.4313 - acc: 0.9034\n",
      "Epoch 50/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4270 - acc: 0.9053lr: 0.02\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.4272 - acc: 0.9052\n",
      "Epoch 51/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4353 - acc: 0.9044lr: 0.02\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.4357 - acc: 0.9043\n",
      "Epoch 52/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4255 - acc: 0.9082lr: 0.02\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.4257 - acc: 0.9081\n",
      "Epoch 53/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4209 - acc: 0.9110lr: 0.02\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.4212 - acc: 0.9109\n",
      "Epoch 54/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4183 - acc: 0.9126lr: 0.02\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.4187 - acc: 0.9125\n",
      "Epoch 55/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4191 - acc: 0.9140lr: 0.02\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.4191 - acc: 0.9140\n",
      "Epoch 56/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4267 - acc: 0.9093lr: 0.02\n",
      "50000/50000 [==============================] - 23s 453us/sample - loss: 1.4269 - acc: 0.9092\n",
      "Epoch 57/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4148 - acc: 0.9159lr: 0.02\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.4151 - acc: 0.9158\n",
      "Epoch 58/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4218 - acc: 0.9150lr: 0.02\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.4222 - acc: 0.9148\n",
      "Epoch 59/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4201 - acc: 0.9151lr: 0.02\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.4201 - acc: 0.9151\n",
      "Epoch 60/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4033 - acc: 0.9214lr: 0.02\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.4033 - acc: 0.9214\n",
      "Epoch 61/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4218 - acc: 0.9150lr: 0.02\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.4217 - acc: 0.9151\n",
      "Epoch 62/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4183 - acc: 0.9188lr: 0.02\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.4185 - acc: 0.9188\n",
      "Epoch 63/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4201 - acc: 0.9164lr: 0.02\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.4203 - acc: 0.9164\n",
      "Epoch 64/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4250 - acc: 0.9168lr: 0.02\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.4254 - acc: 0.9167\n",
      "Epoch 65/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4118 - acc: 0.9199lr: 0.02\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.4117 - acc: 0.9199\n",
      "Epoch 66/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4109 - acc: 0.9212lr: 0.02\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.4110 - acc: 0.9213\n",
      "Epoch 67/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4173 - acc: 0.9186lr: 0.02\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.4178 - acc: 0.9184\n",
      "Epoch 68/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4158 - acc: 0.9194lr: 0.02\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.4159 - acc: 0.9193\n",
      "Epoch 69/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4154 - acc: 0.9202lr: 0.02\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.4156 - acc: 0.9202\n",
      "Epoch 70/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.4180 - acc: 0.9199lr: 0.004\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.4182 - acc: 0.9198\n",
      "Epoch 71/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.2952 - acc: 0.9699lr: 0.004\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.2951 - acc: 0.9699\n",
      "Epoch 72/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.2223 - acc: 0.9945lr: 0.004\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.2224 - acc: 0.9944\n",
      "Epoch 73/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.2003 - acc: 0.9979lr: 0.004\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.2003 - acc: 0.9979\n",
      "Epoch 74/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.1867 - acc: 0.9981lr: 0.004\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.1867 - acc: 0.9981\n",
      "Epoch 75/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.1735 - acc: 0.9990lr: 0.004\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.1736 - acc: 0.9990\n",
      "Epoch 76/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.1624 - acc: 0.9995lr: 0.004\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.1624 - acc: 0.9995\n",
      "Epoch 77/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.1525 - acc: 0.9992lr: 0.004\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.1525 - acc: 0.9992\n",
      "Epoch 78/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.1423 - acc: 0.9994lr: 0.004\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.1422 - acc: 0.9994\n",
      "Epoch 79/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.1336 - acc: 0.9995lr: 0.004\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.1336 - acc: 0.9995\n",
      "Epoch 80/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.1252 - acc: 0.9996lr: 0.004\n",
      "50000/50000 [==============================] - 23s 453us/sample - loss: 1.1252 - acc: 0.9996\n",
      "Epoch 81/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.1160 - acc: 0.9997lr: 0.004\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.1160 - acc: 0.9997\n",
      "Epoch 82/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.1082 - acc: 0.9996lr: 0.004\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.1082 - acc: 0.9996\n",
      "Epoch 83/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.1013 - acc: 0.9998lr: 0.004\n",
      "50000/50000 [==============================] - 23s 452us/sample - loss: 1.1013 - acc: 0.9998\n",
      "Epoch 84/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0928 - acc: 0.9998lr: 0.004\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.0929 - acc: 0.9998\n",
      "Epoch 85/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0854 - acc: 0.9999lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.0854 - acc: 0.9999\n",
      "Epoch 86/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0807 - acc: 0.9998lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 457us/sample - loss: 1.0808 - acc: 0.9998\n",
      "Epoch 87/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0790 - acc: 0.9998lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 458us/sample - loss: 1.0790 - acc: 0.9998\n",
      "Epoch 88/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0773 - acc: 0.9999lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.0773 - acc: 0.9999\n",
      "Epoch 89/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0763 - acc: 0.9998lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 465us/sample - loss: 1.0763 - acc: 0.9998\n",
      "Epoch 90/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0744 - acc: 0.9999lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 456us/sample - loss: 1.0744 - acc: 0.9999\n",
      "Epoch 91/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0734 - acc: 0.9999lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.0734 - acc: 0.9999\n",
      "Epoch 92/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0717 - acc: 0.9999lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 453us/sample - loss: 1.0717 - acc: 0.9999\n",
      "Epoch 93/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0697 - acc: 0.9999lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 453us/sample - loss: 1.0697 - acc: 0.9999\n",
      "Epoch 94/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0696 - acc: 0.9999lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.0697 - acc: 0.9999\n",
      "Epoch 95/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0679 - acc: 1.0000lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.0679 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.9998lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.0665 - acc: 0.9998\n",
      "Epoch 97/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0656 - acc: 0.9998lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.0656 - acc: 0.9998\n",
      "Epoch 98/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0638 - acc: 0.9999lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 455us/sample - loss: 1.0638 - acc: 0.9999\n",
      "Epoch 99/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0624 - acc: 0.9999lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.0624 - acc: 0.9999\n",
      "Epoch 100/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.0611 - acc: 1.0000lr: 0.0008\n",
      "50000/50000 [==============================] - 23s 454us/sample - loss: 1.0611 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c5ab74470>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential()\n",
    "\n",
    "my_init_1=tf.keras.initializers.RandomNormal(mean=0.0, stddev=tf.math.sqrt(2.0/(5*5*192)))\n",
    "my_init_2=tf.keras.initializers.RandomNormal(mean=0.0, stddev=tf.math.sqrt(2.0/160))\n",
    "my_init_3=tf.keras.initializers.RandomNormal(mean=0.0, stddev=tf.math.sqrt(2.0/96))\n",
    "my_init_4=tf.keras.initializers.RandomNormal(mean=0.0, stddev=tf.math.sqrt(2.0/192))\n",
    "my_init_5=tf.keras.initializers.RandomNormal(mean=0.0, stddev=tf.math.sqrt(2.0/(3*3*192)))\n",
    "my_init_dense=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='uniform')\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=5,strides=[1,1],kernel_initializer=my_init_1,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False , input_shape=(32,32,3), padding='same',data_format='channels_last',trainable = False))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05,trainable = False))\n",
    "model.add(tf.keras.layers.ReLU(trainable = False))\n",
    "model.add(tf.keras.layers.Conv2D(160,kernel_size=1,strides=[1,1], kernel_initializer=my_init_2, kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same',trainable = False))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05,trainable = False))\n",
    "model.add(tf.keras.layers.ReLU(trainable = False))\n",
    "model.add(tf.keras.layers.Conv2D(96,kernel_size=1,strides=[1,1], kernel_initializer=my_init_3,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same',trainable = False))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05,trainable = False))\n",
    "model.add(tf.keras.layers.ReLU(trainable = False))\n",
    "model.add(tf.keras.layers.ZeroPadding2D(padding=(1, 1),trainable = False))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2,2),trainable = False))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=5,strides=[1,1], kernel_initializer=my_init_1,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same',trainable = False))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05,trainable = False))\n",
    "model.add(tf.keras.layers.ReLU(trainable = False))\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=1,strides=[1,1], kernel_initializer=my_init_4,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same',trainable = False))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05,trainable = False))\n",
    "model.add(tf.keras.layers.ReLU(trainable = False))\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=1,strides=[1,1], kernel_initializer=my_init_4,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same',trainable = False))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05,trainable = False))\n",
    "model.add(tf.keras.layers.ReLU(trainable = False))\n",
    "model.add(tf.keras.layers.ZeroPadding2D(padding=(1, 1),trainable = False))\n",
    "model.add(tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(2,2),trainable = False))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=3,strides=[1,1], kernel_initializer=my_init_5,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05))\n",
    "model.add(tf.keras.layers.ReLU())\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=1,strides=[1,1], kernel_initializer=my_init_4,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05))\n",
    "model.add(tf.keras.layers.ReLU())\n",
    "model.add(tf.keras.layers.Conv2D(192,kernel_size=1,strides=[1,1], kernel_initializer=my_init_4,kernel_regularizer=tf.keras.regularizers.l2(5e-4), use_bias=False, padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-05))\n",
    "model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax,kernel_initializer=my_init_dense, kernel_regularizer=tf.keras.regularizers.l2(5e-4), bias_initializer='zeros',bias_regularizer=tf.keras.regularizers.l2(5e-4)))\n",
    "\n",
    "my_optimizer=tf.keras.optimizers.SGD(momentum=MOMENTUM, nesterov=True)\n",
    "model.compile(optimizer=my_optimizer,loss=categorical_loss,metrics=['accuracy'])\n",
    "model.fit(x_train, labels, batch_size=128, epochs=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 192)       14400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 32, 32, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 32, 32, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 160)       30720     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 32, 32, 160)       640       \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 32, 32, 160)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 96)        15360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 32, 32, 96)        384       \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 34, 34, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 16, 192)       460800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16, 16, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_12 (ReLU)              (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 16, 16, 192)       36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16, 16, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_13 (ReLU)              (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 192)       36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16, 16, 192)       768       \n",
      "_________________________________________________________________\n",
      "re_lu_14 (ReLU)              (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 18, 18, 192)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 8, 8, 192)         331776    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 8, 8, 192)         768       \n",
      "_________________________________________________________________\n",
      "re_lu_15 (ReLU)              (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 8, 8, 192)         36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 8, 8, 192)         768       \n",
      "_________________________________________________________________\n",
      "re_lu_16 (ReLU)              (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 8, 8, 192)         36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 8, 8, 192)         768       \n",
      "_________________________________________________________________\n",
      "re_lu_17 (ReLU)              (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1930      \n",
      "=================================================================\n",
      "Total params: 1,008,842\n",
      "Trainable params: 408,586\n",
      "Non-trainable params: 600,256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 24s 473us/sample - loss: 6.9396 - acc: 0.1000\n",
      "loss 6.939646385650635 acc 0.10004\n",
      "(10000, 10)\n",
      "10000/10000 [==============================] - 5s 468us/sample - loss: 6.9307 - acc: 0.1001\n",
      "loss 6.9306990501403805 acc 0.1001\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc = model.evaluate(x_train, labels)\n",
    "print(\"loss\", train_loss, \"acc\", train_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
